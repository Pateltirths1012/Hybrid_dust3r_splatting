{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "import pycolmap\n",
    "import cv2\n",
    "import trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('dust3r')\n",
    "sys.path.append('gaussian-splatting')\n",
    "from dust3r.inference import inference as run_depth_inference, load_model\n",
    "from dust3r.utils.image import load_images\n",
    "from dust3r.utils.device import to_numpy\n",
    "from dust3r.image_pairs import make_pairs\n",
    "from dust3r.cloud_opt import global_aligner, GlobalAlignerMode\n",
    "from scene.colmap_loader import rotmat2qvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_4x4(matrix):\n",
    "    return torch.inverse(matrix) if isinstance(matrix, torch.Tensor) else np.linalg.inv(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_numpy(tensor: torch.Tensor):\n",
    "    return tensor.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_image_files(directory: Path):\n",
    "    imgs = sorted([f for f in directory.iterdir() if f.suffix.lower() in ['.png', '.jpg']], \n",
    "                  key=lambda x: int(x.stem))\n",
    "    if not imgs:\n",
    "        raise FileNotFoundError(\"No image files found in the given directory.\")\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_on_depth(img_files, model_ckpt, dev, img_resolution, pair_strateg, batch_sz):\n",
    "    mdl = load_model(model_ckpt, dev)\n",
    "    imgs_tensor = load_images([str(f) for f in img_files], size=img_resolution)\n",
    "    pair_config = make_pairs(imgs_tensor, scene_graph=pair_strateg, prefilter=None, symmetrize=True)\n",
    "    depth_res = run_depth_inference(pair_config, mdl, dev, batch_size=batch_sz)\n",
    "    return imgs_tensor, depth_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_global_alignment(inferred_data, dev, iterations, sched, lr_val, conf_threshold):\n",
    "    align_handle = global_aligner(inferred_data, device=dev, mode=GlobalAlignerMode.PointCloudOptimizer)\n",
    "    align_handle.min_conf_thr = float(align_handle.conf_trf(torch.tensor(conf_threshold)))\n",
    "    align_handle.compute_global_alignment(init=\"mst\", niter=iterations, schedule=sched, lr=lr_val)\n",
    "    return align_handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scene_info(align_obj):\n",
    "    intrinsic_params = tensor_to_numpy(align_obj.get_intrinsics())\n",
    "    cam2world_mats = tensor_to_numpy(align_obj.get_im_poses())\n",
    "    world2cam_mats = invert_4x4(cam2world_mats)\n",
    "    principal_pts = tensor_to_numpy(align_obj.get_principal_points())\n",
    "    focal_lengths = tensor_to_numpy(align_obj.get_focals())\n",
    "    raw_imgs = np.array(align_obj.imgs)\n",
    "    pts3d_data = [p.detach() for p in align_obj.get_pts3d()]\n",
    "    bin_masks = to_numpy(align_obj.get_masks())\n",
    "    return intrinsic_params, cam2world_mats, world2cam_mats, principal_pts, focal_lengths, raw_imgs, pts3d_data, bin_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_geometry(pts3d_arrays, masks_arrays, c2w_matrices):\n",
    "    all_valid_pts = []\n",
    "    for pts, msk in zip(pts3d_arrays, masks_arrays):\n",
    "        chosen = pts[torch.from_numpy(msk).bool()].view(-1, 3)\n",
    "        all_valid_pts.append(chosen)\n",
    "\n",
    "    combined_pts = torch.cat(all_valid_pts, dim=0)\n",
    "    midpoint = combined_pts.mean(dim=0)\n",
    "    scale_factor = torch.norm(combined_pts - midpoint, dim=1).max()\n",
    "\n",
    "    normalized_pts3d = []\n",
    "    mod_c2w = []\n",
    "    midpoint_np = midpoint.numpy()\n",
    "    scale_np = scale_factor.item()\n",
    "\n",
    "    for pts, mat in zip(pts3d_arrays, c2w_matrices):\n",
    "        adjusted_pts = (pts - midpoint) / scale_factor\n",
    "        normalized_pts3d.append(adjusted_pts)\n",
    "        mat_copy = mat.copy()\n",
    "        mat_copy[:3, 3] = (mat_copy[:3, 3] - midpoint_np) / scale_np\n",
    "        mod_c2w.append(mat_copy)\n",
    "\n",
    "    return normalized_pts3d, mod_c2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_scene_output_environment(dest_dir: Path):\n",
    "    if not dest_dir.exists():\n",
    "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_img_dir = dest_dir / 'images'\n",
    "    recon_dir = dest_dir / 'sparse' / '0'\n",
    "    out_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "    recon_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return out_img_dir, recon_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_image_data(scene_imgs, img_dir):\n",
    "    for i, img_arr in enumerate(scene_imgs):\n",
    "        img_save_path = img_dir / f\"{i}.png\"\n",
    "        img_8bit = (img_arr * 255).astype(np.uint8)\n",
    "        final_img = cv2.cvtColor(img_8bit, cv2.COLOR_BGR2RGB)\n",
    "        cv2.imwrite(str(img_save_path), final_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pointcloud(scene_imgs, pts3d_arr, masks_arr):\n",
    "    img_np = to_numpy(scene_imgs)\n",
    "    pts_np = [to_numpy(p) for p in pts3d_arr]\n",
    "    msk_np = to_numpy(masks_arr)\n",
    "\n",
    "    combined_xyz = np.concatenate([p[m].reshape(-1, 3) for p, m in zip(pts_np, msk_np)])\n",
    "    combined_rgb = np.concatenate([im[m].reshape(-1, 3) for im, m in zip(img_np, msk_np)])\n",
    "    xyz_sub = combined_xyz[::3]\n",
    "    rgb_sub = combined_rgb[::3]\n",
    "\n",
    "    normals_arr = np.tile([1, 0, 0], (xyz_sub.shape[0], 1))\n",
    "    pc_obj = trimesh.PointCloud(xyz_sub, colors=(rgb_sub*255).astype(np.uint8))\n",
    "    pc_obj.vertices_normal = normals_arr\n",
    "    return pc_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_initial_view(xyz_coords, focal_list, princ_points, w2c_mats, imgs_arr):\n",
    "    height, width = imgs_arr.shape[1], imgs_arr.shape[2]\n",
    "    fx = focal_list[0][0]\n",
    "    fy = fx\n",
    "    cx, cy = princ_points[0]\n",
    "    rot_mat = w2c_mats[0, :3, :3]\n",
    "    qw, qx, qy, qz = rotmat2qvec(rot_mat)\n",
    "    tx, ty, tz = w2c_mats[0, :3, 3]\n",
    "    R_mat = pycolmap.Rotation3d(np.array([qx, qy, qz, qw])).matrix()\n",
    "    t_vec = np.array([tx, ty, tz]).reshape(3,1)\n",
    "\n",
    "    cam_space = (R_mat @ xyz_coords.T + t_vec).T\n",
    "    x_proj = (cam_space[:,0]*fx / cam_space[:,2]) + cx\n",
    "    y_proj = (cam_space[:,1]*fy / cam_space[:,2]) + cy\n",
    "\n",
    "    forward = cam_space[:,2] > 0\n",
    "    in_frame = (x_proj >= 0) & (x_proj < width) & (y_proj >= 0) & (y_proj < height)\n",
    "    valid_pts = forward & in_frame\n",
    "\n",
    "    return xyz_coords[valid_pts], np.stack([x_proj[valid_pts], y_proj[valid_pts]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_reconstruction(w2c_mats, focal_vals, ppoints, imgs_arr, valid_xyz, init_keypoints, clr_data, recon_dir):\n",
    "    reconstruction = pycolmap.Reconstruction()\n",
    "    h, w = imgs_arr.shape[1], imgs_arr.shape[2]\n",
    "\n",
    "    for i, (focal, pp) in enumerate(zip(focal_vals, ppoints), start=1):\n",
    "        fx = fy = focal[0]\n",
    "        cx, cy = pp\n",
    "        cam = pycolmap.Camera(\n",
    "            model='PINHOLE',\n",
    "            width=w,\n",
    "            height=h,\n",
    "            params=[fx, fy, cx, cy]\n",
    "        )\n",
    "        cam.camera_id = i\n",
    "        reconstruction.add_camera(cam)\n",
    "\n",
    "    first_rot = w2c_mats[0, :3, :3]\n",
    "    qw, qx, qy, qz = rotmat2qvec(first_rot)\n",
    "    r_1 = pycolmap.Rotation3d(np.array([qx, qy, qz, qw]))\n",
    "    t_1 = w2c_mats[0, :3, 3]\n",
    "    first_tf = pycolmap.Rigid3d(r_1, t_1)\n",
    "    first_img = pycolmap.Image(\n",
    "        name=\"0.png\",\n",
    "        cam_from_world=first_tf,\n",
    "        camera_id=1,\n",
    "        keypoints=init_keypoints\n",
    "    )\n",
    "    first_img.image_id = 1\n",
    "    reconstruction.add_image(first_img)\n",
    "\n",
    "    for i in range(1, w2c_mats.shape[0]):\n",
    "        img_id = i + 1\n",
    "        rot_next = w2c_mats[i, :3, :3]\n",
    "        qw, qx, qy, qz = rotmat2qvec(rot_next)\n",
    "        r_next = pycolmap.Rotation3d(np.array([qx, qy, qz, qw]))\n",
    "        t_next = w2c_mats[i, :3, 3]\n",
    "        next_tf = pycolmap.Rigid3d(r_next, t_next)\n",
    "        next_img = pycolmap.Image(\n",
    "            name=f\"{i}.png\",\n",
    "            cam_from_world=next_tf,\n",
    "            camera_id=img_id\n",
    "        )\n",
    "        next_img.image_id = img_id\n",
    "        reconstruction.add_image(next_img)\n",
    "\n",
    "    for idx, (pt_3d, col_v) in enumerate(zip(valid_xyz, clr_data)):\n",
    "        xyz_col = pt_3d.astype(np.float64).reshape((3,1))\n",
    "        rgb_val = col_v[:3].astype(np.uint8).reshape((3,1))\n",
    "        track_el = pycolmap.Track(elements=[pycolmap.TrackElement(image_id=1, point2D_idx=idx)])\n",
    "        p3D_id = reconstruction.add_point3D(xyz=xyz_col, track=track_el, color=rgb_val)\n",
    "        reconstruction.images[1].set_point3D_for_point2D(idx, p3D_id)\n",
    "\n",
    "    reconstruction.write(recon_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading model from dust3r/checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/danny/Documents/cs5330/final_project/dust3r_gs/wild-gaussian-splatting/dust3r/dust3r/inference.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instantiating : AsymmetricCroCo3DStereo(enc_depth=24, dec_depth=12, enc_embed_dim=1024, dec_embed_dim=768, enc_num_heads=16, dec_num_heads=12, pos_embed='RoPE100', patch_embed_cls='PatchEmbedDust3R', img_size=(512, 512), head_type='dpt', output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), landscape_only=False)\n",
      "<All keys matched successfully>\n",
      ">> Loading a list of 3 images\n",
      " - adding images/husky/1.jpg with resolution 288x512 --> 288x512\n",
      " - adding images/husky/2.jpg with resolution 288x512 --> 288x512\n",
      " - adding images/husky/3.jpg with resolution 288x512 --> 288x512\n",
      " (Found 3 images)\n",
      ">> Inference with model on 6 image pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                              | 0/1 [00:00<?, ?it/s]/mnt/c/Users/danny/Documents/cs5330/final_project/dust3r_gs/wild-gaussian-splatting/dust3r/dust3r/inference.py:59: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=bool(use_amp)):\n",
      "/mnt/c/Users/danny/Documents/cs5330/final_project/dust3r_gs/wild-gaussian-splatting/dust3r/dust3r/model.py:161: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/mnt/c/Users/danny/Documents/cs5330/final_project/dust3r_gs/wild-gaussian-splatting/dust3r/dust3r/inference.py:63: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " init edge (2*,0*) score=np.float64(11.55058479309082)\n",
      " init edge (2,1*) score=np.float64(10.795186042785645)\n",
      " init loss = 0.014779966324567795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 250/250 [00:10<00:00, 23.76it/s, lr=1.39474e-06 loss=0.0028977]\n"
     ]
    }
   ],
   "source": [
    "program_params = {\n",
    "    \"target_object\": \"husky\",\n",
    "    \"base_image_dir\": Path('images'),\n",
    "    \"model_checkpoint\": \"dust3r/checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\",\n",
    "    \"inference_device\": \"cuda:0\",\n",
    "    \"input_resolution\": 512,\n",
    "    \"pair_strat\": \"complete\",\n",
    "    \"inference_batch\": 32,\n",
    "    \"global_alignment_iters\": 250,\n",
    "    \"init_learning_rate\": 0.01,\n",
    "    \"min_confidence_thres\": 25,\n",
    "    \"do_normalization\": False,\n",
    "    \"learning_rate_policy\": \"cosine\",\n",
    "    \"scene_output_dir\": Path('results-dust3r')\n",
    "}\n",
    "\n",
    "input_directory = Path.joinpath(program_params['base_image_dir'], program_params['target_object'])\n",
    "image_files = locate_image_files(input_directory)\n",
    "\n",
    "scene_images, depth_inference = run_inference_on_depth(\n",
    "    image_files,\n",
    "    program_params[\"model_checkpoint\"],\n",
    "    program_params[\"inference_device\"],\n",
    "    program_params[\"input_resolution\"],\n",
    "    program_params[\"pair_strat\"],\n",
    "    program_params[\"inference_batch\"]\n",
    ")\n",
    "\n",
    "alignment_op = execute_global_alignment(\n",
    "    depth_inference,\n",
    "    program_params[\"inference_device\"],\n",
    "    program_params[\"global_alignment_iters\"],\n",
    "    program_params[\"learning_rate_policy\"],\n",
    "    program_params[\"init_learning_rate\"],\n",
    "    program_params[\"min_confidence_thres\"]\n",
    ")\n",
    "\n",
    "intrinsics, cam2world, world2cam, p_pts, focs, scene_images, scene_pts3d, scene_msks = extract_scene_info(alignment_op)\n",
    "\n",
    "if program_params[\"do_normalization\"]:\n",
    "    scene_pts3d, cam2world = normalize_geometry(scene_pts3d, scene_msks, cam2world)\n",
    "    world2cam = np.linalg.inv(cam2world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path.joinpath(program_params['scene_output_dir'], program_params['target_object'])\n",
    "images_dir, reconstruction_dir = build_scene_output_environment(out_dir)\n",
    "export_image_data(scene_images, images_dir)\n",
    "\n",
    "pc_obj = create_pointcloud(scene_images, scene_pts3d, scene_msks)\n",
    "xyz_points = pc_obj.vertices\n",
    "col_points = pc_obj.colors\n",
    "\n",
    "valid_xyzs, kpts_first = project_initial_view(xyz_points, focs, p_pts, world2cam, scene_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_reconstruction(world2cam, focs, p_pts, scene_images, valid_xyzs, kpts_first, col_points, reconstruction_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_path = \"gaussian-splatting\"\n",
    "obj_nm = program_params['target_object']\n",
    "d3r_output_dir = str(out_dir)\n",
    "gs_output_dir = str(Path.joinpath(Path(\"results-gaussian-splatting\"), obj_nm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing results-gaussian-splatting/husky\n",
      "Output folder: results-gaussian-splatting/husky [10/12 11:16:34]\n",
      "Tensorboard not available: not logging progress [10/12 11:16:34]\n",
      "Reading camera 3/3 [10/12 11:16:34]\n",
      "Generating ellipse path from 3 camera infos ... [10/12 11:16:34]\n",
      "theta[0] 0.0 [10/12 11:16:34]\n",
      "Loading Training Cameras [10/12 11:16:34]\n",
      "Loading Test Cameras [10/12 11:16:35]\n",
      "Loading Render Cameras [10/12 11:16:35]\n",
      "Number of points at initialisation :  238467 [10/12 11:16:35]\n",
      "Training progress:  23%|▋  | 7000/30000 [02:29<10:07, 37.87it/s, Loss=0.0023073]\n",
      "[ITER 7000] Evaluating train: L1 0.0020739354193210604 PSNR 50.36558990478516 [10/12 11:19:06]\n",
      "\n",
      "[ITER 7000] Saving Gaussians [10/12 11:19:06]\n",
      "Training progress: 100%|██| 30000/30000 [10:36<00:00, 47.15it/s, Loss=0.0011085]\n",
      "\n",
      "[ITER 30000] Evaluating train: L1 0.0009634720627218485 PSNR 55.46571426391602 [10/12 11:27:13]\n",
      "\n",
      "[ITER 30000] Saving Gaussians [10/12 11:27:13]\n",
      "\n",
      "Training complete. [10/12 11:27:16]\n"
     ]
    }
   ],
   "source": [
    "# train gaussian splatting on dust3r results\n",
    "!python \"{gs_path}/train.py\" -s \"{d3r_output_dir}\" -m \"{gs_output_dir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for config file in results-gaussian-splatting/husky/cfg_args\n",
      "Config file found: results-gaussian-splatting/husky/cfg_args\n",
      "Rendering results-gaussian-splatting/husky\n",
      "Loading trained model at iteration 30000 [10/12 11:27:23]\n",
      "Reading camera 3/3 [10/12 11:27:23]\n",
      "Generating ellipse path from 3 camera infos ... [10/12 11:27:23]\n",
      "theta[0] 0.0 [10/12 11:27:23]\n",
      "Loading Training Cameras [10/12 11:27:24]\n",
      "Loading Test Cameras [10/12 11:27:24]\n",
      "Loading Render Cameras [10/12 11:27:24]\n",
      "Rendering progress: 100%|█████████████████████████| 3/3 [00:00<00:00,  5.29it/s]\n",
      "Rendering progress: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# render model\n",
    "!python \"{gs_path}/render.py\" -m \"{gs_output_dir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SIBR] --  INFOS  --:\tInitialization of GLFW\n",
      "[SIBR] --  INFOS  --:\tOpenGL Version: 4.5 (Compatibility Profile) Mesa 24.3.1 - kisak-mesa PPA[major: 4, minor: 5]\n",
      "Number of input Images to read: 3\n",
      "Number of Cameras set up: 3\n",
      "LOADSFM: Try to open /mnt/c/Users/danny/Documents/cs5330/final_project/dust3r_gs/wild-gaussian-splatting/results-dust3r/husky/sparse/0/points3D.bin\n",
      "Num 3D pts 74401\n",
      "[SIBR] --  INFOS  --:\tSfM Mesh '/mnt/c/Users/danny/Documents/cs5330/final_project/dust3r_gs/wild-gaussian-splatting/results-dust3r/husky/sparse/0/points3d.bin successfully loaded.  (74401) vertices detected. Init GL ...\n",
      "[SIBR] --  INFOS  --:\tInit GL mesh complete \n",
      "[SIBR] --  INFOS  --:\tLoading 296149 Gaussian splats\n",
      "[SIBR] --  INFOS  --:\tInitializing Raycaster\n",
      "[SIBR] --  INFOS  --:\tInteractive camera using (0.009,1100) near/far planes.\n",
      "Switched to trackball mode.\n",
      "[SIBR] --  INFOS  --:\tDeinitialization of GLFW\n"
     ]
    }
   ],
   "source": [
    "# interactive viewer\n",
    "!\"./{gs_path}/SIBR_viewers/install/bin/SIBR_gaussianViewer_app\" -m \"{gs_output_dir}\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
